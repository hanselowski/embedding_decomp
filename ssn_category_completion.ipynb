{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from utils.methods import length, unit, cosine, find_vecs, sort_disct, find_sim_cos, hyper_test, find_insts\n",
    "from utils.tree import Tree\n",
    "from random import shuffle\n",
    "from utils.corpuscont import retrieve_corpus\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove():\n",
    "    \"\"\"Glove worde embeddings class\"\"\"\n",
    "    # class to load and process Glove word embeddings\n",
    "    \n",
    "    def __init__(self, glove_filename):\n",
    "        self.loadGloveModel(glove_filename)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.__dict__[key] = value\n",
    "        \n",
    "    def loadGloveModel(self, gloveFile):\n",
    "        f = open(gloveFile,'r')\n",
    "        model = {}\n",
    "\n",
    "        print(f)\n",
    "        cnt = 0\n",
    "        index2word = {}\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            self[word] = embedding\n",
    "            index2word[cnt] = word\n",
    "            cnt += 1\n",
    "            \n",
    "        self.index2word = index2word \n",
    "        \n",
    "        print(\"Done.\",cnt,\" words loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vecs_norm(lst, model):\n",
    "    # function to normalize word vectors\n",
    "    sup_vecs = []\n",
    "    for w in lst: \n",
    "        sup_vecs.append(unit(model[w]))\n",
    "    return sup_vecs\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    \"\"\"semantic tree class\"\"\"\n",
    "        \n",
    "    def __init__(self, model, word_lst, vec_lst, normalize = False):\n",
    "        if vec_lst == None:\n",
    "            if normalize == True:\n",
    "                self.vec_lst = find_vecs_norm(word_lst, model)\n",
    "            else:\n",
    "                self.vec_lst = find_vecs(word_lst, model)\n",
    "            self.word_lst = word_lst\n",
    "        else:\n",
    "            self.vec_lst = vec_lst\n",
    "            self.word_lst = word_lst\n",
    "        \n",
    "        self.model = model\n",
    "        self.root()\n",
    "        \n",
    "    def root(self):\n",
    "        v = self.model['dummy']\n",
    "        H = [0]*v.shape[0]\n",
    "        for vec in self.vec_lst:\n",
    "            H = H + vec\n",
    "        h_u = unit(H)\n",
    "\n",
    "        sup_vec_lens = {}\n",
    "        indzs = range(len(self.word_lst))\n",
    "        for i in indzs: \n",
    "            w = self.word_lst[i]\n",
    "            v = self.vec_lst[i]\n",
    "            v_len = length(v)\n",
    "            cos = cosine(v, H)\n",
    "            h_len = v_len*cos\n",
    "            sup_vec_lens[w] = h_len\n",
    "        self.sup_vec_sort = sort_disct(sup_vec_lens)\n",
    "        self.len = list(self.sup_vec_sort.values())[0]\n",
    "        self.root = self.len * h_u\n",
    "        \n",
    "    def root_unit(self):\n",
    "        v = self.model['dummy']\n",
    "        H = [0]*v.shape[0]\n",
    "        for vec in self.vec_lst:\n",
    "            H = H + unit(vec)\n",
    "        h_u = unit(H)\n",
    "\n",
    "        sup_vec_lens = {}\n",
    "        indzs = range(len(self.word_lst))\n",
    "        for i in indzs: \n",
    "            w = self.word_lst[i]\n",
    "            v = self.vec_lst[i]\n",
    "            v_len = length(v)\n",
    "            cos = cosine(v, H)\n",
    "            h_len = v_len*cos\n",
    "            sup_vec_lens[w] = h_len\n",
    "        self.sup_vec_sort = sort_disct(sup_vec_lens)\n",
    "        self.len = list(self.sup_vec_sort.values())[0]\n",
    "        self.root = self.len * h_u\n",
    "        \n",
    "    def subvec_properties(self):\n",
    "        output = 'sup. vec. order: '\n",
    "        for key in self.sup_vec_sort.keys():\n",
    "            val = sup_vec_sort[key]\n",
    "            output = output + ' ' + key + ' ' + str(val) + '; '\n",
    "        # print(output)\n",
    "        \n",
    "    def hyper_test(self, v, h):\n",
    "        cos = cosine(v, h)\n",
    "        thresh = np.linalg.norm(h)\n",
    "        v_proj = np.linalg.norm(v)*cos\n",
    "        allowance = 0.0000000001 # allowance to include support vectors \n",
    "        instance = 0\n",
    "        if thresh - allowance < v_proj:\n",
    "            instance = 1\n",
    "        return instance\n",
    "    \n",
    "    def find_insts(self):\n",
    "        # print('instances of the root:')\n",
    "        words = []\n",
    "        for i in range(11000):\n",
    "            word = self.model.index2word[i]\n",
    "            instance = self.model[word]\n",
    "            sub_inst = self.hyper_test(instance, self.root)\n",
    "            if sub_inst == 1:\n",
    "                words.append(word)\n",
    "       \n",
    "        len_dict = {}\n",
    "        for w in words:\n",
    "            w_vec = self.model[w]\n",
    "            inst_len = length(w_vec)\n",
    "            cos = cosine(self.root, w_vec)\n",
    "            inst_len_root = cos * inst_len\n",
    "            len_dict[w] = inst_len_root\n",
    "            \n",
    "        sorted_keys = sorted(len_dict.items(), key=operator.itemgetter(1))\n",
    "        reversed_keys = reversed(sorted_keys)\n",
    "        cnt = 0\n",
    "        for k in reversed_keys:\n",
    "            if cnt > 50:\n",
    "                break\n",
    "            cnt += 1\n",
    "    \n",
    "    \n",
    "def find_insts(model, source, normalized = False):\n",
    "    # function to find the children word vectors of the a source sub-vector\n",
    "    \n",
    "    words = []\n",
    "    voc_size = 50000 # here change vocabulary size\n",
    "    try: # handling differenence word2vec and other embeddings\n",
    "        for i in range(voc_size):\n",
    "            word = model.index2word[i]\n",
    "            if normalized:\n",
    "                instance = unit(model[word])\n",
    "            else:\n",
    "                instance = model[word]\n",
    "            sub_inst = hyper_test(instance, source)\n",
    "            if sub_inst == 1:\n",
    "                words.append(word)\n",
    "    except:\n",
    "        vocabulary = model.vocabulary.words\n",
    "        for i in range(voc_size):\n",
    "            if normalized:\n",
    "                instance = unit(model[vocabulary[i]])\n",
    "            else:\n",
    "                instance = model[vocabulary[i]]\n",
    "            \n",
    "            sub_inst = hyper_test(instance, source)\n",
    "            if sub_inst == 1:\n",
    "                words.append(vocabulary[i]) \n",
    "                    \n",
    "    len_dict = {}\n",
    "    for w in words:\n",
    "        w_vec = model[w]\n",
    "        inst_len = length(w_vec)\n",
    "        cos = cosine(source, w_vec)\n",
    "        inst_len_trunk = cos * inst_len\n",
    "        len_dict[w] = inst_len_trunk\n",
    "    sorted_keys = sorted(len_dict.items(), key=operator.itemgetter(1))\n",
    "    reversed_keys = reversed(sorted_keys)\n",
    "    \n",
    "    cnt = 0\n",
    "    lst_out = []\n",
    "    for k in reversed_keys:\n",
    "        lst_out.append(k[0].lower())\n",
    "        if cnt > 1000:\n",
    "            break\n",
    "        cnt += 1\n",
    "    return lst_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cats, inst_pp, itter):\n",
    "    # function to run the category completion experiments\n",
    "\n",
    "    normalization = True\n",
    "    for pp in inst_pp:\n",
    "        precision, recall = 0.0, 0.0 \n",
    "        for cat_upper in cats:\n",
    "            cat = []\n",
    "            [cat.append(t.lower()) for t in cat_upper]\n",
    "            percent = pp\n",
    "            examples = int(round(len(cat)*percent))\n",
    "            \n",
    "            # print('number of examples: ',examples)\n",
    "            # print('examples: ', cat[:examples])\n",
    "            prec, rec, f1 = 0.0, 0.0, 0.0\n",
    "            for i in range(itter):\n",
    "                if dict_low:\n",
    "                    shuffle(cat)\n",
    "                    h_mail = Tree(model, cat[:examples], None, normalization)\n",
    "                else:    \n",
    "                    shuffle(cat_upper)\n",
    "                    h_mail = Tree(model, cat_upper[:examples], None, normalization)\n",
    "\n",
    "                instances_all = find_insts(model, h_mail.root, normalization)\n",
    "                instances = []\n",
    "                [instances.append(el) for i, el in enumerate(instances_all) if el not in instances_all[:i]] # remove duplicates\n",
    "                \n",
    "                hit, mis = 0, 0\n",
    "                if not len(instances) == 0:\n",
    "                    for ins in instances:\n",
    "                        # print(ins)\n",
    "                        if ins in cat:\n",
    "                            hit += 1\n",
    "                        else:\n",
    "                            mis += 1\n",
    "                    prec_tmp = hit/(hit+mis)\n",
    "                else:\n",
    "                    prec_tmp = 0.0\n",
    "                rec_tmp = hit/len(cat)\n",
    "                rec += rec_tmp\n",
    "                prec += prec_tmp\n",
    "\n",
    "            '''\n",
    "            # additional output if needed \n",
    "            print('exaple prediction: ', instances)\n",
    "            print('recall   : ', rec/itter)\n",
    "            print('precision: ', prec/itter)\n",
    "            if rec+prec == 0.0:\n",
    "                print('f1: ', 0.0)\n",
    "            else:\n",
    "                print('f1: ', 2*rec*prec/(rec+prec)/itter)\n",
    "            print('')\n",
    "            '''\n",
    "\n",
    "            recall += rec/itter\n",
    "            precision += prec/itter\n",
    "        print('pp: ', pp)\n",
    "        print('Overall results:')\n",
    "        print('recall   : ', recall/len(cats))\n",
    "        print('precision: ', precision/len(cats))\n",
    "        print('F1: ', round(2*recall*precision/(recall+precision)/len(cats),3) )\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='data/glove/glove.6B.300d.txt' mode='r' encoding='UTF-8'>\n",
      "Done. 400001  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# load corpora \n",
    "dict_low = True\n",
    "path1 = \"datasets/google_cat_corpus.txt\"\n",
    "# path2 = \"datasets/wordnet_cat_corpus.txt\"\n",
    "path3 = \"datasets/closed_cat_corpus.txt\"\n",
    "\n",
    "googe_cat_corpus = retrieve_corpus(path1, dict_low)\n",
    "# wordnet_cat_corpus = retrieve_corpus(path2, dict_low)\n",
    "closed_cat_corpus = retrieve_corpus(path3, dict_low)\n",
    "\n",
    "# load model \n",
    "model = Glove('data/glove/glove.6B.300d.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments on the Closed Category corpus:\n",
      "pp:  0.1\n",
      "Overall results:\n",
      "recall   :  0.1956717611890026\n",
      "precision:  0.9399763818072301\n",
      "F1:  0.324\n",
      " \n",
      "pp:  0.2\n",
      "Overall results:\n",
      "recall   :  0.35051155740810913\n",
      "precision:  0.8722240004849866\n",
      "F1:  0.5\n",
      " \n",
      "pp:  0.3\n",
      "Overall results:\n",
      "recall   :  0.544206559723801\n",
      "precision:  0.7398444092093386\n",
      "F1:  0.627\n",
      " \n",
      "pp:  0.4\n",
      "Overall results:\n",
      "recall   :  0.6974072670624394\n",
      "precision:  0.63174075916704\n",
      "F1:  0.663\n",
      " \n",
      "Experiments on the Google Category corpus:\n",
      "pp:  0.1\n",
      "Overall results:\n",
      "recall   :  0.1630015008347338\n",
      "precision:  0.7868507269541832\n",
      "F1:  0.27\n",
      " \n",
      "pp:  0.2\n",
      "Overall results:\n",
      "recall   :  0.5112322447730616\n",
      "precision:  0.3126515190256547\n",
      "F1:  0.388\n",
      " \n",
      "pp:  0.3\n",
      "Overall results:\n",
      "recall   :  0.6848979498622795\n",
      "precision:  0.21677486226301163\n",
      "F1:  0.329\n",
      " \n",
      "pp:  0.4\n",
      "Overall results:\n",
      "recall   :  0.7791136892441255\n",
      "precision:  0.17166099140955393\n",
      "F1:  0.281\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# run experiments \n",
    "\n",
    "inst_pp = [0.1, 0.2, 0.3, 0.4] # different percentage of reference data 0.1 = 10%, 0.2 = 20% on so on\n",
    "itter = 5 # the experiments are repeated 5 times and averaged to reduce variation \n",
    "\n",
    "print('Experiments on the Closed Category corpus:')\n",
    "run(closed_cat_corpus, inst_pp, itter)\n",
    "\n",
    "print('Experiments on the Google Category corpus:')\n",
    "run(googe_cat_corpus,  inst_pp, itter)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
